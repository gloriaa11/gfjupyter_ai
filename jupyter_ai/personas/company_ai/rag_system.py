import json
import os
import pickle
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import hashlib
import time
from ..jupyternaut.env_config import get_embedding_config, get_embedding_model_priority_list
from .path_config import get_api_knowledge_base_path, get_api_schemas_path, get_extra_docs_path, get_document_paths, validate_paths

try:
    import numpy as np
    from sentence_transformers import SentenceTransformer
    import faiss
    HAS_DEPENDENCIES = True
except ImportError:
    HAS_DEPENDENCIES = False
    print("è­¦å‘Š: ç¼ºå°‘å¿…è¦çš„ä¾èµ–åŒ…ã€‚è¯·å®‰è£…: pip install sentence-transformers faiss-cpu numpy")

class DocumentChunker:
    """æ–‡æ¡£åˆ†å—å™¨ï¼Œå°†é•¿æ–‡æ¡£åˆ†å‰²æˆé€‚åˆå‘é‡åŒ–çš„ç‰‡æ®µ"""
    
    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def chunk_text(self, text: str, title: str = "") -> List[Dict[str, str]]:
        """å°†æ–‡æœ¬åˆ†å‰²æˆå—"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.chunk_size
            
            # å¦‚æœè¿™ä¸æ˜¯æœ€åä¸€å—ï¼Œå°è¯•åœ¨å¥å·æˆ–æ¢è¡Œå¤„åˆ†å‰²
            if end < len(text):
                # å¯»æ‰¾æœ€è¿‘çš„å¥å·æˆ–æ¢è¡Œ
                for i in range(end, max(start + self.chunk_size // 2, start), -1):
                    if text[i] in '.ã€‚\n':
                        end = i + 1
                        break
            
            chunk_text = text[start:end].strip()
            if chunk_text:
                chunks.append({
                    "content": chunk_text,
                    "title": title,
                    "start_pos": start,
                    "end_pos": end
                })
            
            start = end - self.chunk_overlap
            if start >= len(text):
                break
        
        return chunks

class RAGSystem:
    """RAGç³»ç»Ÿï¼šæ–‡æ¡£å‘é‡åŒ–ã€å­˜å‚¨å’Œæ£€ç´¢"""
     
    def __init__(self, vector_db_path: str = None, index_path: str = None):
        """åˆå§‹åŒ–RAGç³»ç»Ÿ"""
        print("æ­£åœ¨åˆå§‹åŒ–RAGç³»ç»Ÿ...")
        
        # éªŒè¯è·¯å¾„é…ç½®
        print("ğŸ” éªŒè¯APIçŸ¥è¯†åº“è·¯å¾„...")
        validate_paths()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.embedding_model = None
        self.vector_db = None
        self.llm_client = None
        self.documents = []
        self.index = None
        
        # è®¾ç½®çŸ¥è¯†åº“æ–‡ä»¶è·¯å¾„åˆ°api_knowledgeç›®å½•
        api_knowledge_base = get_api_knowledge_base_path()
        if vector_db_path is None:
            self.vector_db_path = api_knowledge_base / "vector_db.pkl"
        else:
            self.vector_db_path = Path(vector_db_path)
            
        if index_path is None:
            self.index_path = api_knowledge_base / "faiss_index.bin"
        else:
            self.index_path = Path(index_path)
            
        self.document_hashes = None
        
        print(f"ğŸ“ çŸ¥è¯†åº“æ–‡ä»¶è·¯å¾„: {self.vector_db_path}")
        print(f"ğŸ“ ç´¢å¼•æ–‡ä»¶è·¯å¾„: {self.index_path}")
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        if not self._initialize_embedding_model():
            raise RuntimeError("åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        if not self._initialize_llm_client():
            raise RuntimeError("LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥")
        
        # åŠ è½½æˆ–åˆ›å»ºçŸ¥è¯†åº“
        self._load_or_create_knowledge_base()
        
        print("âœ“ RAGç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
    def _initialize_embedding_model(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ä¸ºè¿œç¨‹APIæ–¹å¼ï¼ŒæŒ‰é¡ºåºå°è¯•ä¸åŒçš„æ¨¡å‹"""
        # Get configuration from environment variables
        embedding_config = get_embedding_config()
        self.embedding_api_url = embedding_config["api_base"]
        self.embedding_api_key = embedding_config["api_key"]
        
        # Get model priority list from environment variables
        model_priority_list = get_embedding_model_priority_list()
        
        # é»˜è®¤å…ˆå°è¯•ç¬¬ä¸€ä¸ªæ¨¡å‹
        self.embedding_model_name = model_priority_list[0]
        print(f"âœ… å°è¯•åˆå§‹åŒ–åµŒå…¥æ¨¡å‹: {self.embedding_model_name}")
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ¨¡å‹å¯ç”¨æ€§æ£€æµ‹é€»è¾‘
        # å®é™…ä½¿ç”¨æ—¶ï¼Œä½ å¯èƒ½éœ€è¦å‘é€æµ‹è¯•è¯·æ±‚æ¥éªŒè¯æ¨¡å‹æ˜¯å¦å¯ç”¨
        # å¦‚æœå½“å‰æ¨¡å‹ä¸å¯ç”¨ï¼Œå¯ä»¥ç»§ç»­å°è¯•åˆ—è¡¨ä¸­çš„ä¸‹ä¸€ä¸ªæ¨¡å‹
        
        print(f"âœ… åµŒå…¥æ¨¡å‹åˆå§‹åŒ–ä¸ºè¿œç¨‹API: {self.embedding_api_url}, model={self.embedding_model_name}")
        return True
    
    def _load_or_create_knowledge_base(self):
        """åŠ è½½æˆ–åˆ›å»ºçŸ¥è¯†åº“"""
        if self.vector_db_path.exists() and self.index_path.exists():
            print("ğŸ” å‘ç°ç°æœ‰çŸ¥è¯†åº“ï¼Œæ­£åœ¨åŠ è½½...")
            print(f"ğŸ“ çŸ¥è¯†åº“æ–‡ä»¶: {self.vector_db_path}")
            print(f"ğŸ“ ç´¢å¼•æ–‡ä»¶: {self.index_path}")
            try:
                self._load_knowledge_base()
                print(f"âœ… çŸ¥è¯†åº“åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(self.documents)} ä¸ªæ–‡æ¡£")
                return
            except Exception as e:
                print(f"âŒ çŸ¥è¯†åº“åŠ è½½å¤±è´¥: {e}")
                print("ğŸ”„ å°†é‡æ–°åˆ›å»ºçŸ¥è¯†åº“...")
        
        print("ğŸ“š æ²¡æœ‰æ‰¾åˆ°ç°æœ‰çŸ¥è¯†åº“ï¼Œå°†åˆ›å»ºæ–°çš„...")
        self.documents = self._load_documents()
        if self.documents:
            self._create_knowledge_base()
        else:
            print("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ–‡æ¡£")
    def _load_knowledge_base(self):
        """åŠ è½½ç°æœ‰çŸ¥è¯†åº“"""
        # åŠ è½½æ–‡æ¡£
        with open(self.vector_db_path, 'rb') as f:
            data = pickle.load(f)
            self.documents = data['documents']
            self.document_hashes = data.get('hashes', {})
            created_at = data.get('created_at', 0)
            model_name = data.get('model_name', 'unknown')
            
            print(f"ğŸ“… çŸ¥è¯†åº“åˆ›å»ºæ—¶é—´: {time.ctime(created_at)}")
            print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model_name}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰å˜åŒ–
        # è·³è¿‡æºæ–‡ä»¶å˜åŒ–æ£€æµ‹ï¼Œç›´æ¥ä½¿ç”¨ç°æœ‰çŸ¥è¯†åº“
        print("âœ… ç›´æ¥ä½¿ç”¨ç°æœ‰çŸ¥è¯†åº“ï¼Œè·³è¿‡æºæ–‡ä»¶å˜åŒ–æ£€æµ‹")
        
        # åŠ è½½FAISSç´¢å¼•
        self.index = faiss.read_index(str(self.index_path))
        print(f"âœ… çŸ¥è¯†åº“åŠ è½½å®Œæˆï¼ŒåŒ…å« {len(self.documents)} ä¸ªæ–‡æ¡£")
    
    def _check_files_changed(self):
        """æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦æœ‰å˜åŒ–"""
        if not self.document_hashes:
            return True
            
        for doc in self.documents:
            source = doc['source']
            if source in self.document_hashes:
                try:
                    if Path(source).exists():
                        current_hash = self._get_file_hash(Path(source))
                        if current_hash != self.document_hashes[source]:
                            print(f"ğŸ”„ æ–‡ä»¶å·²å˜åŒ–: {source}")
                            return True
                    else:
                        print(f"âš ï¸ æ–‡ä»¶å·²åˆ é™¤: {source}")
                        return True
                except:
                    print(f"âš ï¸ æ— æ³•æ£€æŸ¥æ–‡ä»¶: {source}")
                    return True
        return False
    
    def _save_knowledge_base(self):
        """ä¿å­˜çŸ¥è¯†åº“"""
        # è®¡ç®—æ–‡æ¡£å“ˆå¸Œå€¼
        if self.document_hashes is None:
            self.document_hashes = {}
            for doc in self.documents:
                source = doc['source']
                if source in self.document_hashes:
                    continue
                try:
                    if Path(source).exists():
                        self.document_hashes[source] = self._get_file_hash(Path(source))
                except:
                    self.document_hashes[source] = "unknown"
        
        # ä¿å­˜æ–‡æ¡£å’Œå“ˆå¸Œå€¼
        with open(self.vector_db_path, 'wb') as f:
            pickle.dump({
                'documents': self.documents,
                'hashes': self.document_hashes,
                'created_at': time.time(),
                'model_name': self.embedding_model_name
            }, f)
        
        # ä¿å­˜FAISSç´¢å¼•
        if self.index is not None:
            faiss.write_index(self.index, str(self.index_path))
    
    def _create_knowledge_base(self):
        """åˆ›å»ºæ–°çš„çŸ¥è¯†åº“"""
        print("æ­£åœ¨åˆ›å»ºæ–°çš„çŸ¥è¯†åº“...")
        
        # åŠ è½½æ–‡æ¡£
        self._load_documents()
        
        # å‘é‡åŒ–æ–‡æ¡£
        if self.documents:
            self._vectorize_documents()
            self._save_knowledge_base()
            print(f"çŸ¥è¯†åº“åˆ›å»ºå®Œæˆï¼ŒåŒ…å« {len(self.documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        else:
            print("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ–‡æ¡£")
    
    def _load_documents(self):
        """åŠ è½½æ–‡æ¡£ï¼Œé™åˆ¶æ•°é‡é¿å…å†…å­˜æº¢å‡º"""
        documents = []
        max_documents = 500  # é™åˆ¶æœ€å¤§æ–‡æ¡£æ•°é‡
        loaded_count = 0
        
        # ä½¿ç”¨ç»å¯¹è·¯å¾„è·å–æ–‡æ¡£è·¯å¾„
        document_paths = get_document_paths()
        
        for path in document_paths:
            if loaded_count >= max_documents:
                print(f"âš ï¸ å·²è¾¾åˆ°æœ€å¤§æ–‡æ¡£æ•°é‡é™åˆ¶ ({max_documents})ï¼Œåœæ­¢åŠ è½½")
                break
                
            doc_path = Path(path)
            if doc_path.exists():
                print(f"ğŸ“ å‘ç°æ–‡æ¡£è·¯å¾„: {doc_path}")
                
                if doc_path.is_file():
                    # å•ä¸ªæ–‡ä»¶
                    try:
                        if doc_path.suffix.lower() == '.json':
                            # JSONæ–‡ä»¶
                            with open(doc_path, 'r', encoding='utf-8') as f:
                                import json
                                data = json.load(f)
                                # å°†JSONè½¬æ¢ä¸ºæ–‡æœ¬
                                content = json.dumps(data, ensure_ascii=False, indent=2)
                                documents.append({
                                    'content': content,
                                    'source': str(doc_path)
                                })
                                loaded_count += 1
                                print(f"âœ… åŠ è½½JSONæ–‡ä»¶: {doc_path.name} (æ€»è®¡: {loaded_count})")
                        else:
                            # å…¶ä»–æ–‡æœ¬æ–‡ä»¶
                            with open(doc_path, 'r', encoding='utf-8') as f:
                                content = f.read()
                                documents.append({
                                    'content': content,
                                    'source': str(doc_path)
                                })
                                loaded_count += 1
                                print(f"âœ… åŠ è½½æ–‡æœ¬æ–‡ä»¶: {doc_path.name} (æ€»è®¡: {loaded_count})")
                    except Exception as e:
                        print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {doc_path}: {e}")
                        
                elif doc_path.is_dir():
                    # ç›®å½•
                    print(f"ğŸ“‚ æ‰«æç›®å½•: {doc_path}")
                    for file_path in doc_path.rglob('*'):
                        if loaded_count >= max_documents:
                            print(f"âš ï¸ å·²è¾¾åˆ°æœ€å¤§æ–‡æ¡£æ•°é‡é™åˆ¶ ({max_documents})ï¼Œåœæ­¢æ‰«æç›®å½•")
                            break
                            
                        if file_path.is_file() and file_path.suffix.lower() in ['.txt', '.md', '.json', '.py', '.html']:
                            try:
                                if file_path.suffix.lower() == '.json':
                                    # JSONæ–‡ä»¶
                                    with open(file_path, 'r', encoding='utf-8') as f:
                                        import json
                                        data = json.load(f)
                                        content = json.dumps(data, ensure_ascii=False, indent=2)
                                        documents.append({
                                            'content': content,
                                            'source': str(file_path)
                                        })
                                        loaded_count += 1
                                        print(f"âœ… åŠ è½½JSONæ–‡ä»¶: {file_path.name} (æ€»è®¡: {loaded_count})")
                                else:
                                    # å…¶ä»–æ–‡æœ¬æ–‡ä»¶
                                    with open(file_path, 'r', encoding='utf-8') as f:
                                        content = f.read()
                                        documents.append({
                                            'content': content,
                                            'source': str(file_path)
                                        })
                                        loaded_count += 1
                                        print(f"âœ… åŠ è½½æ–‡ä»¶: {file_path.name} (æ€»è®¡: {loaded_count})")
                            except Exception as e:
                                print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            else:
                print(f"âš ï¸ æ–‡æ¡£è·¯å¾„ä¸å­˜åœ¨: {doc_path}")
        
        print(f"ğŸ“š æ€»å…±åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
        # ä¿®å¤ï¼šå°†æ–‡æ¡£èµ‹å€¼ç»™self.documents
        self.documents = documents
        return documents
    
    def _get_file_hash(self, file_path: Path) -> str:
        """è·å–æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼"""
        with open(file_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    
    def _vectorize_documents(self):
        """å‘é‡åŒ–æ–‡æ¡£ï¼Œåˆ†æ‰¹å¤„ç†é¿å…å†…å­˜æº¢å‡º"""
        if not self.documents:
            return
        import requests
        
        texts = [doc["content"] for doc in self.documents]
        print(f"æ­£åœ¨é€šè¿‡APIç”Ÿæˆæ–‡æ¡£å‘é‡, æ€»æ•°: {len(texts)}")
        
        # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹æœ€å¤š50ä¸ªæ–‡æ¡£
        batch_size = 50
        all_embeddings = []
        
        headers = {"Authorization": f"Bearer {self.embedding_api_key}", "Content-Type": "application/json"}
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(texts) + batch_size - 1) // batch_size
            
            print(f"ğŸ”„ å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹ï¼ŒåŒ…å« {len(batch_texts)} ä¸ªæ–‡æ¡£")
            
            payload = {
                "model": self.embedding_model_name,
                "input": batch_texts
            }
            
            try:
                print(f"ğŸ” DEBUG: å‘é€åµŒå…¥APIè¯·æ±‚åˆ°: {self.embedding_api_url}")
                print(f"ğŸ” DEBUG: ä½¿ç”¨æ¨¡å‹: {self.embedding_model_name}")
                print(f"ğŸ” DEBUG: ä½¿ç”¨APIå¯†é’¥: {self.embedding_api_key[:8]}...{self.embedding_api_key[-4:] if len(self.embedding_api_key) > 12 else '***'}")
                
                resp = requests.post(self.embedding_api_url, headers=headers, json=payload, timeout=120)
                print(f"ğŸ” DEBUG: åµŒå…¥APIå“åº”çŠ¶æ€ç : {resp.status_code}")
                print(f"ğŸ” DEBUG: åµŒå…¥APIå“åº”å†…å®¹é•¿åº¦: {len(resp.text)} å­—ç¬¦")
                
                if resp.status_code != 200:
                    print(f"âŒ åµŒå…¥APIè¯·æ±‚å¤±è´¥: {resp.status_code} - {resp.text[:200]}...")
                    raise Exception(f"åµŒå…¥APIè¯·æ±‚å¤±è´¥: {resp.status_code} - {resp.text[:200]}")
                    
                # æ£€æŸ¥å“åº”å¤§å°ï¼Œé¿å…å†…å­˜æº¢å‡º
                if len(resp.text) > 100 * 1024 * 1024:  # 100MB
                    print(f"âš ï¸ å“åº”è¿‡å¤§ ({len(resp.text)} å­—ç¬¦)ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                    continue
                
                data = resp.json()
                
                # æ£€æŸ¥å“åº”ä¸­æ˜¯å¦åŒ…å«é”™è¯¯ä¿¡æ¯
                if "code" in data and data["code"] != 200:
                    error_msg = data.get("msg", "æœªçŸ¥é”™è¯¯")
                    print(f"âŒ åµŒå…¥APIä¸šåŠ¡é”™è¯¯: {data['code']} - {error_msg}")
                    raise Exception(f"åµŒå…¥APIä¸šåŠ¡é”™è¯¯: {data['code']} - {error_msg}")
                
                if not data or "data" not in data:
                    print(f"âŒ åµŒå…¥APIå“åº”æ ¼å¼é”™è¯¯: {data}")
                    raise Exception(f"åµŒå…¥APIå“åº”æ ¼å¼é”™è¯¯: {data}")
                    
                batch_embeddings = [item["embedding"] for item in data["data"]]
                all_embeddings.extend(batch_embeddings)
                print(f"âœ… ç¬¬ {batch_num} æ‰¹å¤„ç†å®Œæˆï¼Œè·å¾— {len(batch_embeddings)} ä¸ªå‘é‡")
                
            except Exception as e:
                print(f"âŒ ç¬¬ {batch_num} æ‰¹APIåµŒå…¥è·å–å¤±è´¥: {e}")
                # ç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹ï¼Œè€Œä¸æ˜¯å®Œå…¨å¤±è´¥
                continue
        
        if not all_embeddings:
            raise Exception("æ‰€æœ‰æ‰¹æ¬¡çš„åµŒå…¥è·å–éƒ½å¤±è´¥äº†")
            
        vectors = np.array(all_embeddings, dtype="float32")
        print(f"âœ… æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œæ€»å…±è·å¾— {len(all_embeddings)} ä¸ªå‘é‡ï¼Œshape={vectors.shape}")
        # åˆ›å»ºFAISSç´¢å¼•
        dimension = vectors.shape[1]
        self.index = faiss.IndexFlatIP(dimension)
        faiss.normalize_L2(vectors)
        self.index.add(vectors)
        print(f"å‘é‡åŒ–å®Œæˆï¼Œç´¢å¼•åŒ…å« {self.index.ntotal} ä¸ªå‘é‡")
        
        # ä¿å­˜çŸ¥è¯†åº“åˆ°æœ¬åœ°
        print("ğŸ’¾ ä¿å­˜çŸ¥è¯†åº“åˆ°æœ¬åœ°...")
        self._save_knowledge_base()
        print(f"âœ… çŸ¥è¯†åº“å·²ä¿å­˜åˆ°: {self.vector_db_path}")
        print(f"âœ… ç´¢å¼•å·²ä¿å­˜åˆ°: {self.index_path}")
    def _load_documents(self):
        """
        åŠ è½½æ–‡æ¡£ï¼š
        1) ä» api_knowledge/api_schemas.json è¯»å–ç”± .whl æå–çš„ API æ¨¡å¼ï¼ˆå‡½æ•°/ç±»ï¼‰ï¼Œä¸€é¡¹ä¸€å—ï¼›
        2) æ‰«æé¢å¤–ç›®å½•ï¼ˆ.txt/.md/.json/.py/.htmlï¼‰ï¼Œåˆ†å—ååªæŒ‘é€‰æœ€å¯èƒ½æœ‰ç”¨çš„ Top-3 ç‰‡æ®µåŠ å…¥ã€‚
        """
        from pathlib import Path
        import json

        def _safe_json_load(path: Path):
            """å®¹é”™è¯»å– JSONï¼šæ”¯æŒå•ä¸ª JSON æ•°ç»„ã€å•ä¸ªå¯¹è±¡ï¼Œæˆ–è¢«å¤šæ¬¡ append çš„â€œå¤šæ®µ JSONâ€"""
            text = path.read_text(encoding="utf-8")
            try:
                return json.loads(text)
            except json.JSONDecodeError:
                dec = json.JSONDecoder()
                idx, n = 0, len(text)
                all_items = []
                while idx < n:
                    while idx < n and text[idx].isspace():
                        idx += 1
                    if idx >= n:
                        break
                    try:
                        obj, end = dec.raw_decode(text, idx)
                        idx = end
                        if isinstance(obj, list):
                            all_items.extend(obj)
                        elif isinstance(obj, dict):
                            all_items.append(obj)
                        else:
                            all_items.append(obj)
                    except json.JSONDecodeError:
                        break
                return all_items

        def _infer_signature(name: str, params: dict, ret: dict) -> str:
            """ä» parameters/return ç²—ç•¥ç”Ÿæˆç­¾åå­—ç¬¦ä¸²"""
            props = (params or {}).get("properties", {}) or {}
            required = set((params or {}).get("required", []) or [])
            parts = []
            for pname, meta in props.items():
                if pname == "self":
                    continue
                ptype = str(meta.get("type", "any"))
                default = meta.get("default", None)
                if pname in required or default in (None, "None"):
                    parts.append(f"{pname}: {ptype}")
                else:
                    parts.append(f"{pname}: {ptype}={default}")
            rtype = (ret or {}).get("type", "any")
            return f"{name}({', '.join(parts)}) -> {rtype}"

        def _render_function_doc(name: str, desc: str, params: dict, ret: dict) -> str:
            """å°†å‡½æ•°ä¿¡æ¯æ¸²æŸ“ä¸ºç»Ÿä¸€æ–‡æœ¬"""
            sig = _infer_signature(name, params, ret)
            props = (params or {}).get("properties", {}) or {}
            required = set((params or {}).get("required", []) or [])
            lines = [
                f"[FUNCTION] {name}",
                "[DESCRIPTION]",
                (desc or "").strip() or "No description available.",
                "",
                "[SIGNATURE]",
                sig,
                "",
                "[PARAMETERS]",
            ]
            if props:
                for k, v in props.items():
                    if k == "self":
                        continue
                    t = str(v.get("type", "any"))
                    req = "required" if k in required else "optional"
                    default = v.get("default", None)
                    if default not in (None, "None"):
                        lines.append(f"- {k} ({t}, {req}, default={default})")
                    else:
                        lines.append(f"- {k} ({t}, {req})")
            else:
                lines.append("- (no parameters)")
            lines.extend(["", "[RETURN]"])
            lines.append(f"- type: {str((ret or {}).get('type', 'any'))}")
            return "\n".join(lines).strip()

        def _render_class_doc(cls_name: str, desc: str, methods: list, properties: list) -> str:
            """æ¸²æŸ“ç±»ä¿¡æ¯ä¸ºå¯æ£€ç´¢æ–‡æœ¬"""
            lines = [
                f"[CLASS] {cls_name}",
                "[DESCRIPTION]",
                (desc or "").strip() or "No description available.",
            ]
            if properties:
                lines.extend(["", "[PROPERTIES]"])
                for p in properties:
                    lines.append(f"- {p.get('name','unknown')}: {p.get('description','').strip() or 'No description'}")
            if methods:
                lines.extend(["", "[METHODS]"])
                for m in methods:
                    lines.append(f"- {m.get('name','unknown')}")
            return "\n".join(lines).strip()

        def _score_chunk(chunk: dict) -> float:
            """å¯å‘å¼ä¸ºåˆ†å—æ‰“åˆ†ï¼šåå‘ç¤ºä¾‹/ä»£ç /å‡½æ•°å®šä¹‰/ç”¨æ³•ç­‰å†…å®¹"""
            title = (chunk.get("title") or "").lower()
            text = (chunk.get("content") or "")
            text_l = text.lower()

            # å…³é”®è¯ä¸ä»£ç çº¿ç´¢
            kw = [
                "example", "examples", "ç¤ºä¾‹", "æ ·ä¾‹", "ç”¨æ³•", "usage", "è°ƒç”¨",
                "å‚æ•°", "returns", "è¿”å›", "raise", "error", "å¼‚å¸¸",
                "def ", "class ", "```", "->", "return ", "import ", "from "
            ]
            score = 0.0
            for k in kw:
                score += text_l.count(k) * (2.0 if k.strip() in {"def", "class"} else 1.0)

            # æ ‡é¢˜å‘½ä¸­é¢å¤–åŠ åˆ†
            if any(k in title for k in ["example", "examples", "ç¤ºä¾‹", "æ ·ä¾‹", "usage", "ç”¨æ³•"]):
                score += 3.0

            # ä»£ç é£æ ¼å­—ç¬¦æ¯”ä¾‹ï¼ˆè¶Šâ€œåƒä»£ç â€è¶Šé«˜ï¼‰
            code_signals = "(){}[]:;=.".join([""])  # ç®€å•èšåˆ
            code_hits = sum(text.count(s) for s in ["def ", "class ", "return", "(", ")", ":", "{", "}", "```"])
            score += 0.15 * code_hits

            # é•¿åº¦æƒ©ç½š/å¥–åŠ±ï¼šåå¥½ 200~1200 å­—çš„ä¸­ç­‰ç‰‡æ®µ
            L = len(text)
            if 200 <= L <= 1200:
                score += 1.5
            elif L > 3000:
                score -= 0.5  # è¶…é•¿ç•¥é™

            return score

        documents = []

        # 1) è¯»å– API æ¨¡å¼ï¼ˆç”± .whl æå–ç”Ÿæˆï¼‰
        api_json_path = get_api_schemas_path()

        if api_json_path.exists():
            try:
                data = _safe_json_load(api_json_path)
                items = []
                if isinstance(data, list):
                    items = data
                elif isinstance(data, dict):
                    for key in ("functions", "classes", "apis", "items"):
                        v = data.get(key)
                        if isinstance(v, list):
                            items.extend(v)
                    if not items:
                        items = [data]
                else:
                    items = [data]

                for it in items:
                    it_type = (it or {}).get("type", "").lower()
                    if it_type == "function":
                        name = it.get("name", "unknown")
                        desc = it.get("description", "")
                        params = it.get("parameters", {})
                        ret = it.get("return") or it.get("returns") or {}
                        content = _render_function_doc(name, desc, params, ret)
                        documents.append({
                            "id": f"api::{name}",
                            "title": name,
                            "type": "api_function",
                            "priority": 1.0,
                            "source": str(api_json_path),
                            "content": content
                        })

                    elif it_type == "class":
                        cls_name = it.get("name", "UnknownClass")
                        desc = it.get("description", "")
                        methods = it.get("methods", []) or []
                        properties = it.get("properties", []) or []

                        class_content = _render_class_doc(cls_name, desc, methods, properties)
                        documents.append({
                            "id": f"api_class::{cls_name}",
                            "title": cls_name,
                            "type": "api_class",
                            "priority": 0.9,
                            "source": str(api_json_path),
                            "content": class_content
                        })

                        for m in methods:
                            if not isinstance(m, dict):
                                continue
                            if m.get("name", "").startswith("_"):
                                continue
                            m_name = f"{cls_name}.{m.get('name','unknown')}"
                            m_desc = m.get("description", "")
                            m_params = m.get("parameters", {})
                            m_ret = m.get("return") or m.get("returns") or {}
                            m_content = _render_function_doc(m_name, m_desc, m_params, m_ret)
                            documents.append({
                                "id": f"api::{m_name}",
                                "title": m_name,
                                "type": "api_function",
                                "priority": 1.0,
                                "source": str(api_json_path),
                                "content": m_content
                            })
                    else:
                        documents.append({
                            "id": f"api_misc::{it.get('name','unknown')}",
                            "title": it.get("name", "unknown"),
                            "type": "api_misc",
                            "priority": 0.8,
                            "source": str(api_json_path),
                            "content": json.dumps(it, ensure_ascii=False, indent=2)
                        })
            except Exception as e:
                print(f"âŒ åŠ è½½APIçŸ¥è¯†æ–‡ä»¶å¤±è´¥: {e}")
        else:
            print("âš ï¸ æœªæ‰¾åˆ° api_schemas.json")

        # 2) å…¶å®ƒæ–‡æ¡£é€‰æ‹©æ€§åˆ†å—ï¼ˆTop-3 ç‰‡æ®µæŒ‘é€‰ï¼‰
        extra_dir = get_extra_docs_path()

        if extra_dir.exists() and extra_dir.is_dir():
            for file_path in extra_dir.rglob('*'):
                if file_path.is_file() and file_path.suffix.lower() in ['.txt', '.md', '.json', '.py', '.html']:
                    try:
                        content = file_path.read_text(encoding='utf-8', errors='ignore')
                        chunker = DocumentChunker(chunk_size=500, chunk_overlap=50)
                        chunks = chunker.chunk_text(content, title=file_path.name)

                        # ä»…é€‰æ‹©æœ€å¯èƒ½æœ‰ç”¨çš„ Top-3 ç‰‡æ®µ
                        scored = sorted(chunks, key=_score_chunk, reverse=True)
                        topk = scored[:min(3, len(scored))]

                        for idx, chunk in enumerate(topk):
                            documents.append({
                                "id": f"{file_path.name}::top{idx}",
                                "content": chunk['content'],
                                "title": chunk.get('title', file_path.name),
                                "type": "document",
                                "priority": 0.6,
                                "source": str(file_path)
                            })
                    except Exception as e:
                        print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

        self.documents = documents
        return documents

    def update_knowledge_base(self):
        """æ›´æ–°çŸ¥è¯†åº“"""
        print("æ­£åœ¨æ›´æ–°çŸ¥è¯†åº“...")
        
        # é‡æ–°åŠ è½½æ–‡æ¡£
        self._load_documents()
        
        # é‡æ–°å‘é‡åŒ–
        if self.documents:
            self._vectorize_documents()
            self._save_knowledge_base()
            print(f"çŸ¥è¯†åº“æ›´æ–°å®Œæˆï¼ŒåŒ…å« {len(self.documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        else:
            print("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ–‡æ¡£")
    
    def get_knowledge_summary(self) -> str:
        """è·å–çŸ¥è¯†åº“æ‘˜è¦"""
        if not self.documents:
            return "çŸ¥è¯†åº“ä¸ºç©º"
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_chunks = len(self.documents)
        api_functions = len([d for d in self.documents if d.get("type") == "api_function"])
        api_classes = len([d for d in self.documents if d.get("type") == "api_class"])
        documents = len([d for d in self.documents if d.get("type") == "document"])
        
        summary = f"## çŸ¥è¯†åº“æ‘˜è¦\n\n"
        summary += f"- æ€»æ–‡æ¡£ç‰‡æ®µæ•°: {total_chunks}\n"
        summary += f"- APIå‡½æ•°: {api_functions}\n"
        summary += f"- APIç±»: {api_classes}\n"
        summary += f"- æ–‡æ¡£ç‰‡æ®µ: {documents}\n\n"
        
        # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
        summary += "### ç¤ºä¾‹APIå‡½æ•°\n"
        api_examples = [d for d in self.documents if d.get("type") == "api_function"][:5]
        for doc in api_examples:
            summary += f"- {doc['title']}: {doc['content'][:100]}...\n"
        
        summary += "\n### ç¤ºä¾‹æ–‡æ¡£\n"
        doc_examples = [d for d in self.documents if d.get("type") == "document"][:3]
        for doc in doc_examples:
            summary += f"- {doc['title']}: {doc['content'][:100]}...\n"
        
        return summary 
    def _initialize_llm_client(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        try:
            import sys
            import os
            
            # ä½¿ç”¨ç¡¬ç¼–ç è·¯å¾„
            project_root = r"C:\Users\dc-develop-2\Desktop\work\pre\code\jupyter-ai\jupyter-ai"
            sys.path.insert(0, project_root)
            
            from llm_client import LLMClient
            from api_config import API_KEY
            
            self.llm_client = LLMClient(api_key=API_KEY)
            print("âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    def search(self, query: str, top_k: int = 3, max_content_length: int = 500, max_segments: int = 2):
        """
        åŸºäº TF-IDF çš„è½»é‡ç›¸ä¼¼åº¦æ£€ç´¢ï¼š
        - å¯¹æ¯ä¸ªæ–‡æ¡£æŒ‰æ®µè½/ä»£ç å—åˆ†æ®µï¼Œè®¡ç®—æ®µè½ä¸æŸ¥è¯¢çš„ç›¸ä¼¼åº¦
        - æ¯ä¸ªæ–‡æ¡£ä»…é€‰å‡ºç›¸ä¼¼åº¦æœ€é«˜çš„ max_segments ä¸ªç‰‡æ®µï¼ŒæŒ‰ max_content_length æ‹¼æ¥
        - æ–‡æ¡£å¾—åˆ† = é€‰ä¸­ç‰‡æ®µç›¸ä¼¼åº¦çš„å¹³å‡å€¼ * ç±»å‹/ä¼˜å…ˆçº§åŠ æƒ
        - è¿”å›æŒ‰å¾—åˆ†æ’åºçš„ top_k æ–‡æ¡£ï¼ˆæ¯æ¡åŒ…å«å‹ç¼©åçš„ content ä¸ similarity_scoreï¼‰
        """
        import re, math, copy

        if not getattr(self, "documents", None):
            if hasattr(self, "_load_documents"):
                print("[DEBUG] æ–‡æ¡£æœªåŠ è½½ï¼Œè°ƒç”¨ _load_documents()")
                self._load_documents()
            else:
                raise AttributeError("RAGSystem ç¼ºå°‘ documentsï¼Œè¯·å…ˆå®ç°å¹¶è°ƒç”¨ _load_documents()")

        # ---------- åŸºç¡€å·¥å…· ----------
        def normalize_text(t: str) -> str:
            return (t or "").strip()

        def tokenize(t: str):
            """æ”¯æŒè‹±æ–‡å•è¯ã€æ•°å­—ã€ä»¥åŠé€å­—çš„ä¸­æ—¥éŸ©ç»Ÿä¸€è¡¨æ„æ–‡å­—"""
            t = t.lower()
            words = re.findall(r"[a-z_][a-z0-9_]+", t)
            nums  = re.findall(r"\d+(?:\.\d+)?", t)
            cjk   = re.findall(r"[\u4e00-\u9fff]", t)
            return words + nums + cjk

        def split_segments(text: str):
            """
            å°†æ–‡æ¡£åˆ‡æˆè¯­ä¹‰æ®µï¼š
            - å…ˆæŒ‰ä¸‰å¼•å·/ä»£ç å›´æ /ç©ºè¡Œåˆ‡
            - å†å¯¹è¶…é•¿æ®µæŒ‰ ~400 å­—åšäºŒæ¬¡åˆ‡åˆ†
            """
            text = normalize_text(text)
            if not text:
                return []

            # ä»£ç å›´æ ä¼˜å…ˆ
            fence_split = re.split(r"(```.*?```)", text, flags=re.S)
            raw_segs = []
            for part in fence_split:
                if not part.strip():
                    continue
                if part.startswith("```") and part.endswith("```"):
                    raw_segs.append(part)
                else:
                    raw_segs.extend([s for s in re.split(r"\n\s*\n", part) if s.strip()])

            # äºŒæ¬¡é•¿åº¦åˆ‡åˆ†ï¼ˆ~400 å­—ï¼‰
            final = []
            for seg in raw_segs:
                s = seg.strip()
                if len(s) <= 600:
                    final.append(s)
                    continue
                # æŒ‰å¥å·/æ¢è¡Œåšè½¯åˆ‡ï¼Œå†å…œåº•ç¡¬åˆ‡
                pieces = re.split(r"(?<=[ã€‚ï¼ï¼Ÿ.!?])\s+|\n{1,}", s)
                buf = ""
                for p in pieces:
                    if not p.strip():
                        continue
                    if len(buf) + len(p) + 1 <= 600:
                        buf = (buf + "\n" + p) if buf else p
                    else:
                        if buf:
                            final.append(buf)
                        buf = p
                if buf:
                    final.append(buf)

                # å…œåº•ç¡¬åˆ‡ï¼Œé˜²æ­¢æç«¯é•¿æ®µ
                hard_final = []
                for s2 in final[-3:]:  # ä»…å¯¹åˆšåŠ å…¥çš„æ®µå…œåº•
                    if len(s2) <= 800:
                        hard_final.append(s2)
                    else:
                        for i in range(0, len(s2), 700):
                            hard_final.append(s2[i:i+700])
                final = final[:-3] + hard_final if len(final) >= 3 else hard_final
            return final

        # ---------- æ„å»ºåˆ†æ®µè¯­æ–™ä¸ DF ----------
        corpus_segments = []  # [(doc_idx, seg_idx, seg_text, type, priority)]
        for i, d in enumerate(self.documents):
            content = normalize_text(d.get("content", ""))
            if not content:
                continue
            segs = split_segments(content)
            typ = d.get("type", "document")
            pri = float(d.get("priority", 1.0))
            for j, s in enumerate(segs):
                corpus_segments.append((i, j, s, typ, pri))

        if not corpus_segments:
            print("[DEBUG] æ²¡æœ‰å¯æ£€ç´¢çš„åˆ†æ®µè¯­æ–™")
            return []

        # è®¡ç®— DF
        df = {}
        for _, _, seg_text, _, _ in corpus_segments:
            toks = set(tokenize(seg_text))
            for t in toks:
                df[t] = df.get(t, 0) + 1
        N = len(corpus_segments)
        idf = {t: math.log(1.0 + N / (dfc + 1.0)) for t, dfc in df.items()}

        # æŸ¥è¯¢å‘é‡
        q_tokens = tokenize(query or "")
        if not q_tokens:
            print("[DEBUG] æŸ¥è¯¢ä¸ºç©ºæˆ–æœªèƒ½äº§ç”Ÿæœ‰æ•ˆ token")
            return []

        def tfidf_vec(tokens):
            tf = {}
            for t in tokens:
                tf[t] = tf.get(t, 0.0) + 1.0
            # å½’ä¸€åŒ–
            length = sum(v * v for v in tf.values()) ** 0.5 or 1.0
            # ä¹˜ä»¥ idf
            vec = {t: (tf[t] / length) * idf.get(t, math.log(1.0 + N)) for t in tf}
            return vec

        q_vec = tfidf_vec(q_tokens)

        def cos_sim(vec_a, vec_b):
            # ç¨€ç–å‘é‡ç‚¹ç§¯
            if len(vec_a) > len(vec_b):
                vec_a, vec_b = vec_b, vec_a
            s = 0.0
            for k, va in vec_a.items():
                vb = vec_b.get(k)
                if vb:
                    s += va * vb
            # å½’ä¸€ï¼ˆvec_b å·²åœ¨ tfidf_vec é‡Œåšäº† L2 è¿‘ä¼¼ï¼‰
            return s

        # é¢„è®¡ç®—æ¯ä¸ªåˆ†æ®µå‘é‡
        seg_vecs = []
        for tup in corpus_segments:
            seg_vecs.append(tfidf_vec(tokenize(tup[2])))

        # ---------- æ¯æ–‡æ¡£æŒ‘é€‰ top æ®µè½ ----------
        # ç±»å‹åŠ æƒ
        type_weight = {
            "api_function": 1.3,
            "api_class":    1.15,
            "code_block":   1.05,
            "example_snippet": 1.05,
            "document":     1.0,
            "api_misc":     0.95
        }

        from collections import defaultdict
        per_doc_candidates = defaultdict(list)  # doc_idx -> [(seg_text, sim, typ, pri)]

        for (i, j, seg_text, typ, pri), seg_vec in zip(corpus_segments, seg_vecs):
            base_sim = cos_sim(q_vec, seg_vec)
            boost = type_weight.get(typ, 1.0) * float(pri or 1.0)
            sim = base_sim * boost
            per_doc_candidates[i].append((seg_text, sim, typ, pri))

        # ä¸ºæ¯ä¸ªæ–‡æ¡£é€‰å‡ºç›¸ä¼¼åº¦æœ€é«˜çš„ max_segments ä¸ªç‰‡æ®µï¼Œå¹¶æŒ‰é•¿åº¦é¢„ç®—æ‹¼æ¥
        scored_docs = []
        for doc_idx, cand in per_doc_candidates.items():
            if not cand:
                continue
            cand.sort(key=lambda x: x[1], reverse=True)
            chosen = cand[:max_segments]

            selected_content = []
            total_len = 0
            for seg_text, seg_sim, _typ, _pri in chosen:
                seg_text = seg_text.strip()
                if not seg_text:
                    continue
                if total_len + len(seg_text) <= max_content_length:
                    selected_content.append(seg_text)
                    total_len += len(seg_text)
                else:
                    remaining = max_content_length - total_len
                    if remaining > 50:  # é¿å…è¿‡çŸ­ç¢ç‰‡
                        selected_content.append(seg_text[:remaining] + " â€¦")
                        total_len += remaining
                    break

            if not selected_content:
                # å›é€€ï¼šç›´æ¥æˆªå–åŸæ–‡å‰ max_content_length
                raw = normalize_text(self.documents[doc_idx].get("content", ""))[:max_content_length]
                if raw:
                    selected_content = [raw + (" â€¦" if len(raw) == max_content_length else "")]
                else:
                    continue

            # æ–‡æ¡£å¾—åˆ†ï¼šå·²é€‰ç‰‡æ®µç›¸ä¼¼åº¦çš„å‡å€¼ï¼ˆæˆ–æœ€å¤§å€¼ä¹Ÿå¯ï¼‰
            doc_score = sum(s for _, s, _, _ in chosen) / max(1, len(chosen))

            # ç»„è£…ç»“æœæ¡ç›®ï¼ˆä¸ç ´ååŸ self.documentsï¼‰
            d = self.documents[doc_idx]
            new_doc = {
                "id": d.get("id"),
                "title": d.get("title"),
                "type": d.get("type"),
                "priority": d.get("priority", 1.0),
                "source": d.get("source"),
                "content": "\n\n".join(selected_content),
                "similarity_score": float(doc_score),
            }
            scored_docs.append(new_doc)

        # ---------- å…¨å±€æ’åºå¹¶è£å‰ª ----------
        scored_docs.sort(key=lambda x: x["similarity_score"], reverse=True)
        results = scored_docs[:max(1, top_k)]

        # è°ƒè¯•è¾“å‡º
        print(f"[DEBUG] RAGSystem.search å®Œæˆï¼Œè¿”å› {len(results)} ä¸ªç»“æœ")
        for idx, r in enumerate(results):
            clen = len(r.get("content") or "")
            print(f"[DEBUG] Top{idx+1} æ ‡é¢˜: {r.get('title')} | ç±»å‹: {r.get('type')} | åˆ†æ•°: {r['similarity_score']:.4f} | é•¿åº¦: {clen}")

        return results
